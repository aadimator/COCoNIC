{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_n_mkdir(dir_path: str):\n",
    "    \"\"\"Remove and make directory.\"\"\"\n",
    "    if os.path.isdir(dir_path):\n",
    "        shutil.rmtree(dir_path)\n",
    "    os.makedirs(dir_path)\n",
    "\n",
    "def save_json(d: Dict, file: str):\n",
    "  file = Path(file)\n",
    "  file.parents[0].mkdir(exist_ok=True, parents=True)\n",
    "  with open(file, 'w') as f:\n",
    "    json.dump(d, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def create_splits(\n",
    "  patch_info_file: str, \n",
    "  seed: int = 5, \n",
    "  n_splits: int = 10, \n",
    "  train_size: float = 0.8, \n",
    "  test_size: float = 0.2\n",
    "  ) -> List[Dict]:\n",
    "  \"\"\"Creates splits of the dataset, given the `patch_info` file.\n",
    "  This uses the same defaults that were used for HoVerNet baseline splits.\n",
    "\n",
    "  Args:\n",
    "      patch_info_file (str): path of the 'patch_info.json' file.\n",
    "      seed (int, optional): seed for reproducibility. Defaults to 5.\n",
    "      n_splits (int, optional): number of splits/folds to create. Defaults to 10.\n",
    "      train_size (float, optional): train percentage. Defaults to 0.8.\n",
    "      test_size (float, optional): test percentage. Defaults to 0.2.\n",
    "\n",
    "  Returns:\n",
    "      List[Dict]: list contaning different folds for 'train' and 'valid'\n",
    "  \"\"\"  \n",
    "  info = pd.read_csv(patch_info_file)\n",
    "  file_names = np.squeeze(info.to_numpy()).tolist()\n",
    "\n",
    "  img_sources = [v.split('-')[0] for v in file_names]\n",
    "  img_sources = np.unique(img_sources)\n",
    "\n",
    "  cohort_sources = [v.split('_')[0] for v in img_sources]\n",
    "  _, cohort_sources = np.unique(cohort_sources, return_inverse=True)\n",
    "\n",
    "  splitter = StratifiedShuffleSplit(\n",
    "      n_splits=n_splits,\n",
    "      train_size=train_size,\n",
    "      test_size=test_size,\n",
    "      random_state=seed\n",
    "  )\n",
    "\n",
    "  splits = []\n",
    "  split_generator = splitter.split(img_sources, cohort_sources)\n",
    "  for train_indices, valid_indices in split_generator:\n",
    "      train_cohorts = img_sources[train_indices]\n",
    "      valid_cohorts = img_sources[valid_indices]\n",
    "      assert np.intersect1d(train_cohorts, valid_cohorts).size == 0\n",
    "      train_names = [\n",
    "          file_name\n",
    "          for file_name in file_names\n",
    "          for source in train_cohorts\n",
    "          if source == file_name.split('-')[0]\n",
    "      ]\n",
    "      valid_names = [\n",
    "          file_name\n",
    "          for file_name in file_names\n",
    "          for source in valid_cohorts\n",
    "          if source == file_name.split('-')[0]\n",
    "      ]\n",
    "      train_names = np.unique(train_names)\n",
    "      valid_names = np.unique(valid_names)\n",
    "      print(f'Train: {len(train_names):04d} - Valid: {len(valid_names):04d}')\n",
    "      assert np.intersect1d(train_names, valid_names).size == 0\n",
    "      # train_indices = [file_names.index(v) for v in train_names]\n",
    "      # valid_indices = [file_names.index(v) for v in valid_names]\n",
    "      splits.append({\n",
    "          'train': train_names,\n",
    "          'valid': valid_names\n",
    "      })\n",
    "  return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"../dataset\" \n",
    "\n",
    "OUT_DIR = 'output'\n",
    "\n",
    "images_json = f'{OUT_DIR}/images.json'\n",
    "instances_json = f'{OUT_DIR}/instances.json'\n",
    "panoptic_json = f'{OUT_DIR}/panoptic.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 3963 - Valid: 1018\n",
      "Train: 4053 - Valid: 0928\n",
      "Train: 3952 - Valid: 1029\n",
      "Train: 3988 - Valid: 0993\n",
      "Train: 3997 - Valid: 0984\n",
      "Train: 4002 - Valid: 0979\n",
      "Train: 3894 - Valid: 1087\n",
      "Train: 4012 - Valid: 0969\n",
      "Train: 3988 - Valid: 0993\n",
      "Train: 3964 - Valid: 1017\n"
     ]
    }
   ],
   "source": [
    "info_path = f\"{data_root}/patch_info.csv\"\n",
    "\n",
    "splits = create_splits(info_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_folds(splits: List[Dict], json_path: str, folds_dir: str, data_type: str):\n",
    "  \"\"\"Saves the folds from `splits` into respective json files in `folds_dir`/`data_type`/*\n",
    "\n",
    "  Args:\n",
    "      splits (List[Dict]): splits generated using the `create_split` function.\n",
    "      json_path (str): path to the input json file\n",
    "      folds_dir (str): directory to store the folds json files\n",
    "      data_type (str): could be either 'instances' or 'panoptic'\n",
    "  \"\"\"  \n",
    "  # assert data_type in ['instances', 'panoptic']\n",
    "\n",
    "  with open(json_path, 'r') as json_file:\n",
    "    info = json.load(json_file)\n",
    "\n",
    "  images_pd = pd.DataFrame(info['images'])\n",
    "  ann_pd = pd.DataFrame(info['annotations'])\n",
    "\n",
    "  for split in range(len(splits)):\n",
    "    for mode in ['train', 'valid']:\n",
    "      images_pd_mode = images_pd.loc[images_pd['id'].isin(splits[split][mode])]\n",
    "      ann_pd_mode = ann_pd.loc[ann_pd['image_id'].isin(splits[split][mode])]\n",
    "\n",
    "      info_mode = info\n",
    "      info_mode['images'] = images_pd_mode.to_dict(orient='records')\n",
    "      info_mode['annotations'] = ann_pd_mode.to_dict(orient='records')\n",
    "\n",
    "      save_json(info_mode, f'{folds_dir}/{data_type}/{mode}/fold_{split}.json')\n",
    "      print(f'Saved {folds_dir}/{data_type}/{mode}/fold_{split}.json')\n",
    "\n",
    "  print(f\"JSON files for {len(splits)} folds created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_n_mkdir(f'{OUT_DIR}/folds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved output/folds/instances/train/fold_0.json\n",
      "Saved output/folds/instances/valid/fold_0.json\n",
      "Saved output/folds/instances/train/fold_1.json\n",
      "Saved output/folds/instances/valid/fold_1.json\n",
      "Saved output/folds/instances/train/fold_2.json\n",
      "Saved output/folds/instances/valid/fold_2.json\n",
      "Saved output/folds/instances/train/fold_3.json\n",
      "Saved output/folds/instances/valid/fold_3.json\n",
      "Saved output/folds/instances/train/fold_4.json\n",
      "Saved output/folds/instances/valid/fold_4.json\n",
      "Saved output/folds/instances/train/fold_5.json\n",
      "Saved output/folds/instances/valid/fold_5.json\n",
      "Saved output/folds/instances/train/fold_6.json\n",
      "Saved output/folds/instances/valid/fold_6.json\n",
      "Saved output/folds/instances/train/fold_7.json\n",
      "Saved output/folds/instances/valid/fold_7.json\n",
      "Saved output/folds/instances/train/fold_8.json\n",
      "Saved output/folds/instances/valid/fold_8.json\n",
      "Saved output/folds/instances/train/fold_9.json\n",
      "Saved output/folds/instances/valid/fold_9.json\n",
      "JSON files for 10 folds created successfully.\n"
     ]
    }
   ],
   "source": [
    "save_folds(splits, instances_json, f'{OUT_DIR}/folds', 'instances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved output/folds/panoptic/train/fold_0.json\n",
      "Saved output/folds/panoptic/valid/fold_0.json\n",
      "Saved output/folds/panoptic/train/fold_1.json\n",
      "Saved output/folds/panoptic/valid/fold_1.json\n",
      "Saved output/folds/panoptic/train/fold_2.json\n",
      "Saved output/folds/panoptic/valid/fold_2.json\n",
      "Saved output/folds/panoptic/train/fold_3.json\n",
      "Saved output/folds/panoptic/valid/fold_3.json\n",
      "Saved output/folds/panoptic/train/fold_4.json\n",
      "Saved output/folds/panoptic/valid/fold_4.json\n",
      "Saved output/folds/panoptic/train/fold_5.json\n",
      "Saved output/folds/panoptic/valid/fold_5.json\n",
      "Saved output/folds/panoptic/train/fold_6.json\n",
      "Saved output/folds/panoptic/valid/fold_6.json\n",
      "Saved output/folds/panoptic/train/fold_7.json\n",
      "Saved output/folds/panoptic/valid/fold_7.json\n",
      "Saved output/folds/panoptic/train/fold_8.json\n",
      "Saved output/folds/panoptic/valid/fold_8.json\n",
      "Saved output/folds/panoptic/train/fold_9.json\n",
      "Saved output/folds/panoptic/valid/fold_9.json\n",
      "JSON files for 10 folds created successfully.\n"
     ]
    }
   ],
   "source": [
    "save_folds(splits, panoptic_json, f'{OUT_DIR}/folds', 'panoptic')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d5db21d37d8d35a94a14b53cee058c92fcd33b109f8c7e081551be0d2c0fb88"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('capnic': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
